{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiMy+hIvNG1KvB3A7K0yPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Umanginigam/Langgraph/blob/main/Langgraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for building the agentic workflow.\n",
        "# langgraph: workflow engine for agents\n",
        "# langchain: abstraction for LLMs and tools\n",
        "# transformers: for open-source language models\n",
        "# accelerate: optimized hardware usage\n",
        "!pip install --quiet langgraph langchain transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itj1j230k2NJ",
        "outputId": "59651e04-35ea-4f52-800b-f62b52719a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llm = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_new_tokens=256)\n",
        "\n"
      ],
      "metadata": {
        "id": "r_xlP8Gy_0v-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d169849f-48ab-4b90-d459-ed9768a75195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plan_agent_node(state):\n",
        "    # Check if there are existing tasks using Pydantic-style access\n",
        "    if not state.tasks:\n",
        "        # Initial planning from user query\n",
        "        prompt = f\"Break this user request into numbered subtasks:\\nUser query: {state.user_query}\"\n",
        "    else:\n",
        "        # Refine existing tasks based on feedback\n",
        "        prompt = f\"Refine these subtasks based on the feedback:\\nTasks: {state.tasks}\\nFeedback: {state.feedback}\"\n",
        "\n",
        "    # Call the language model (LLM) to generate tasks\n",
        "    response = llm(prompt)[0]['generated_text']\n",
        "\n",
        "    # Split response into a clean list of tasks\n",
        "    tasks = [line.strip() for line in response.split('\\n') if line.strip()]\n",
        "\n",
        "    # Return a partial state update (LangGraph will merge this into the full state)\n",
        "    return {\n",
        "        \"tasks\": tasks,\n",
        "        \"feedback\": []  # Reset feedback after planning\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "xE3Mw_z3IQce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tool_agent_node(state):\n",
        "    results = []\n",
        "\n",
        "    # ✅ Loop through each subtask and generate step-by-step solutions\n",
        "    for task in state.tasks:\n",
        "        # Construct the prompt for the LLM\n",
        "        prompt = f\"Step-by-step solution for: {task}\"\n",
        "\n",
        "        # Call the language model (assuming llm returns a list of completions)\n",
        "        response = llm(prompt)[0]['generated_text']\n",
        "\n",
        "        # Collect the generated result\n",
        "        results.append(response)\n",
        "\n",
        "    # ✅ Return a dictionary that matches the expected keys in the state schema\n",
        "    return {\n",
        "        \"results\": results\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ml8md21_IUJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reflection_node(state):\n",
        "    feedback = []\n",
        "\n",
        "    # ✅ Access results using dot notation\n",
        "    for result in state.results:\n",
        "        # Prompt the LLM to evaluate each result and provide feedback\n",
        "        prompt = f\"Evaluate this result and suggest improvement if needed:\\n{result}\"\n",
        "        fb = llm(prompt)[0]['generated_text']\n",
        "        feedback.append(fb)\n",
        "\n",
        "    # ✅ Return the updated feedback as expected by LangGraph\n",
        "    return {\n",
        "        \"feedback\": feedback\n",
        "    }\n"
      ],
      "metadata": {
        "id": "1abzbNj1IWBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def needs_more_refinement(feedback: list) -> str:\n",
        "    for f in feedback:\n",
        "        if \"improve\" in f.lower() or \"not clear\" in f.lower():\n",
        "            return \"continue\"\n",
        "    return \"END\"\n"
      ],
      "metadata": {
        "id": "hkKsjwITIYqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "# ✅ Define schema using Pydantic (not a plain dict)\n",
        "class AgentState(BaseModel):\n",
        "    user_query: str\n",
        "    tasks: List[str] = []\n",
        "    results: List[str] = []\n",
        "    feedback: List[str] = []\n",
        "\n",
        "# ✅ Build graph using AgentState\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes and edges as before\n",
        "graph.add_node(\"PlanAgent\", plan_agent_node)\n",
        "graph.add_node(\"ToolAgent\", tool_agent_node)\n",
        "graph.add_node(\"Reflection\", reflection_node)\n",
        "\n",
        "graph.set_entry_point(\"PlanAgent\")\n",
        "graph.add_edge(\"PlanAgent\", \"ToolAgent\")\n",
        "graph.add_edge(\"ToolAgent\", \"Reflection\")\n",
        "\n",
        "# Conditional edge based on feedback\n",
        "graph.add_conditional_edges(\n",
        "    \"Reflection\",\n",
        "    lambda state: needs_more_refinement(state.feedback),\n",
        "    {\n",
        "        \"continue\": \"PlanAgent\",\n",
        "        \"END\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()\n"
      ],
      "metadata": {
        "id": "r2ojtHhkIcmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = AgentState(\n",
        "    user_query=\"Generate a data report for sales in 2024 and visualize trends using Python\"\n",
        ")\n",
        "\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "# 👇 Add this to see the output\n",
        "print(\"🧠 Final Agent State:\\n\")\n",
        "print(final_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMqyrOUzIfin",
        "outputId": "cfe4a9d0-cc6d-4bf8-83e6-ed54257dddec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Final Agent State:\n",
            "\n",
            "{'user_query': 'Generate a data report for sales in 2024 and visualize trends using Python', 'tasks': ['Data report for sales in 2024 and visualize trends using Python'], 'results': ['Solution: Data report for sales in 2024 and visualize trends using Python'], 'feedback': ['This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 compared to the same period last year. This results in a 32% increase in sales in 2024 ']}\n"
          ]
        }
      ]
    }
  ]
}